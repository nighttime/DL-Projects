# Feed-Forward
This network architecture is designed using 2 dense layers with a sigmoid activation of layer 1 and dropout. It uses the Adam optimizer with a reduced learning rate to best fit the training data.
